{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env CarRacing-v3 with observations tensor([96, 96,  3]) and actions tensor([3])\n",
      "###\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kagi/dreamer/dreamer.py:280: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpointPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  12000: worldModelLoss, criticLoss, actorLoss, reward =   1.1752,   0.0461,   0.5383, -2.77\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_12000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_12000_reward_-9.mp4\n",
      "i  12500: worldModelLoss, criticLoss, actorLoss, reward =   1.1310,   0.0208,   1.5298, -14.97\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_12500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_12500_reward_-15.mp4\n",
      "i  13000: worldModelLoss, criticLoss, actorLoss, reward =   1.1380,   0.0093,   0.2456, -12.60\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_13000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_13000_reward_-8.mp4\n",
      "i  13500: worldModelLoss, criticLoss, actorLoss, reward =   1.2663,   0.1043,  -1.1769, 20.44\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_13500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_13500_reward_8.mp4\n",
      "i  14000: worldModelLoss, criticLoss, actorLoss, reward =   1.1312,   0.0260,   0.0456, -17.98\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_14000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_14000_reward_-18.mp4\n",
      "i  14500: worldModelLoss, criticLoss, actorLoss, reward =   1.1095,   0.0024,   0.9893, -18.79\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_14500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_14500_reward_-18.mp4\n",
      "i  15000: worldModelLoss, criticLoss, actorLoss, reward =   1.1086,   0.0023,   0.3722, -18.99\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_15000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_15000_reward_-18.mp4\n",
      "i  15500: worldModelLoss, criticLoss, actorLoss, reward =   1.1202,   0.0023,  -0.5248, -14.22\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_15500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_15500_reward_-15.mp4\n",
      "i  16000: worldModelLoss, criticLoss, actorLoss, reward =   1.1155,   0.0066,  -1.1464, -18.79\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_16000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_16000_reward_-18.mp4\n",
      "i  16500: worldModelLoss, criticLoss, actorLoss, reward =   1.1081,   0.0028,   0.9460, -18.01\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_16500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_16500_reward_-18.mp4\n",
      "i  17000: worldModelLoss, criticLoss, actorLoss, reward =   1.1067,   0.0010,   0.8057, -19.17\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_17000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_17000_reward_-18.mp4\n",
      "i  17500: worldModelLoss, criticLoss, actorLoss, reward =   1.1063,   0.0031,   1.5291, -18.15\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_17500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_17500_reward_-18.mp4\n",
      "i  18000: worldModelLoss, criticLoss, actorLoss, reward =   1.1056,   0.0010,   0.8396, -17.72\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_18000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_18000_reward_-18.mp4\n",
      "i  18500: worldModelLoss, criticLoss, actorLoss, reward =   1.1102,   0.0018,  -0.5893, -12.92\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_18500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_18500_reward_-12.mp4\n",
      "i  19000: worldModelLoss, criticLoss, actorLoss, reward =   1.1286,   0.0074,  -0.2818, -12.92\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_19000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_19000_reward_-11.mp4\n",
      "i  19500: worldModelLoss, criticLoss, actorLoss, reward =   1.1131,   0.0013,   0.8794, -15.01\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_19500.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_19500_reward_-16.mp4\n",
      "i  20000: worldModelLoss, criticLoss, actorLoss, reward =   1.1085,   0.0005,   0.1961, -15.16\n",
      "Saved html plot to plots/CarRacing-v3_BATCH_TEST_2_20000.html\n",
      "Saved video to videos/CarRacing-v3_BATCH_TEST_2_20000_reward_-16.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     58\u001b[0m     action \u001b[38;5;241m=\u001b[39m dreamer\u001b[38;5;241m.\u001b[39mact(observation, reset\u001b[38;5;241m=\u001b[39m(stepCount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     observation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mtranspose(observation, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     61\u001b[0m     stepCount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/envs/box2d/car_racing.py:563\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    566\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/envs/box2d/car_racing.py:658\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_image_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (VIDEO_W, VIDEO_H))\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTATE_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTATE_H\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/gymnasium/envs/box2d/car_racing.py:789\u001b[0m, in \u001b[0;36mCarRacing._create_image_array\u001b[0;34m(self, screen, size)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_image_array\u001b[39m(\u001b[38;5;28mself\u001b[39m, screen, size):\n\u001b[0;32m--> 789\u001b[0m     scaled_screen \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoothscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    791\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(scaled_screen)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    792\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "from dreamer import *\n",
    "import random\n",
    "torch.set_printoptions(threshold=2000, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(threshold=2000, linewidth=200)\n",
    "\n",
    "environmentName = \"CarRacing-v3\"\n",
    "renderMode = None\n",
    "numUpdates = 10000\n",
    "episodesBeforeStart = 20\n",
    "playInterval = 10\n",
    "stepCountLimit = 256\n",
    "bufferSize = 20\n",
    "resume = True\n",
    "saveMetrics = True\n",
    "saveCheckpoints = True\n",
    "runName = f\"{environmentName}_BATCH_TEST_2\"\n",
    "checkpointToLoad = f\"checkpoints/{runName}_11500\"\n",
    "metricsFilename = f\"metrics/{runName}\"\n",
    "plotFilename = f\"plots/{runName}\"\n",
    "videoFilename = f\"videos/{runName}\"\n",
    "saveMetricsInterval = 10\n",
    "checkpointInterval = 500\n",
    "numNewEpisodePlay = 1\n",
    "seed = 2\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(environmentName, render_mode=renderMode)\n",
    "observationShape = torch.tensor(env.observation_space.shape)\n",
    "actionSize = torch.tensor(env.action_space.shape) if hasattr(env.action_space, 'shape') else np.array([env.action_space.n])\n",
    "print(f\"Env {environmentName} with observations {observationShape} and actions {actionSize}\\n###\\n\")\n",
    "dreamer = Dreamer()\n",
    "\n",
    "episodeBuffer = EpisodeBuffer(size=bufferSize)\n",
    "\n",
    "if resume:\n",
    "    dreamer.loadCheckpoint(checkpointToLoad)\n",
    "    start = dreamer.totalUpdates\n",
    "else:\n",
    "    start = 0\n",
    "\n",
    "for i in range(start - episodesBeforeStart, start + numUpdates + 1):\n",
    "    for _ in range(numNewEpisodePlay):\n",
    "        if i % playInterval == 0 or i < start:\n",
    "            observation, info = env.reset(seed=seed + abs(i))\n",
    "            observation = torch.from_numpy(np.transpose(observation, (2, 0, 1))).unsqueeze(0).to(device).float()/255.0\n",
    "            observations, actions, rewards, dones = [observation], [], [], []\n",
    "            stepCount, totalReward, done = 1, 0, False\n",
    "            while not done:\n",
    "                action = dreamer.act(observation, reset=(stepCount == 1)).view(-1)\n",
    "                observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "                observation = torch.from_numpy(np.transpose(observation, (2, 0, 1))).unsqueeze(0).to(device).float()/255.0\n",
    "                stepCount += 1\n",
    "                done = terminated or truncated or stepCount >= stepCountLimit\n",
    "                totalReward += reward\n",
    "                \n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                # dones.append(done)\n",
    "\n",
    "            episodeBuffer.addEpisode(torch.stack(observations).squeeze(1),\n",
    "                                    torch.stack(actions).to(device),\n",
    "                                    torch.tensor(rewards).to(device))\n",
    "\n",
    "    if i > start:\n",
    "        selectedEpisodeObservations, selectedEpisodeActions, selectedEpisodeRewards = episodeBuffer.sampleEpisodes(dreamer.worldModelBatchSize)\n",
    "        sampledFullStates, worldModelLoss, reconstructionLoss, rewardPredictionLoss, klLoss = dreamer.trainWorldModel(selectedEpisodeObservations, selectedEpisodeActions, selectedEpisodeRewards)\n",
    "        criticLoss, actorLoss, valueEstimate = dreamer.trainActorCritic(sampledFullStates)\n",
    "\n",
    "    if i % saveMetricsInterval == 0 and i > start and saveMetrics:\n",
    "        saveLossesToCSV(metricsFilename, {\n",
    "            \"i\": i,\n",
    "            \"worldModelLoss\": worldModelLoss,\n",
    "            \"reconstructionLoss\": reconstructionLoss,\n",
    "            \"rewardPredictionLoss\": rewardPredictionLoss,\n",
    "            \"klLoss\": klLoss,\n",
    "            \"criticLoss\": criticLoss,\n",
    "            \"actorLoss\": actorLoss,\n",
    "            \"valueEstimate\": valueEstimate,\n",
    "            \"totalReward\": totalReward})\n",
    "        \n",
    "        # print(f\"\\nnewest actions:\\n{episodeBuffer.getNewestEpisode()[1][:5]}\")\n",
    "\n",
    "    if i % checkpointInterval == 0 and i > start and saveCheckpoints:\n",
    "        print(f\"i {i:6}: worldModelLoss, criticLoss, actorLoss, reward = {worldModelLoss:8.4f}, {criticLoss:8.4f}, {actorLoss:8.4f}, {totalReward:.2f}\")\n",
    "        dreamer.totalUpdates = i\n",
    "        dreamer.saveCheckpoint(f\"checkpoints/{runName}_{i}\")\n",
    "        plotMetrics(metricsFilename, show=False, save=True, savePath=f\"{plotFilename}_{i}\") # TODO: plot should replace the unnecessary previous file\n",
    "        saveVideoFromGymEnv(dreamer, environmentName, f\"{videoFilename}_{i}\", frameLimit=stepCountLimit)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in dreamer.actor.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out rollout of the world model\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Initialize your Dreamer model and device here\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the start image\n",
    "start_image = np.array(Image.open(\"startImage2.png\"))  # Replace with uploaded image path\n",
    "start_image_tensor = torch.from_numpy(np.transpose(start_image, (2, 0, 1))).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "# Initialize the rollout\n",
    "recurrent_state, latent_state = dreamer.rolloutInitialize(start_image_tensor)\n",
    "\n",
    "# Define dark mode colors\n",
    "BG_COLOR = \"#333333\"\n",
    "FG_COLOR = \"#DDDDDD\"\n",
    "SLIDER_COLOR = \"#555555\"\n",
    "SLIDER_THUMB_COLOR = \"#AAAAAA\"\n",
    "BUTTON_COLOR = \"#444444\"\n",
    "BUTTON_HOVER_COLOR = \"#666666\"\n",
    "\n",
    "# GUI setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Dreamer Rollout Interface\")\n",
    "root.configure(bg=BG_COLOR)\n",
    "root.attributes('-fullscreen', True)  # Fullscreen mode\n",
    "root.bind(\"<Escape>\", lambda event: root.attributes(\"-fullscreen\", False))  # Exit fullscreen with ESC\n",
    "\n",
    "# Position window on primary monitor (top left corner)\n",
    "root.geometry(f\"{root.winfo_screenwidth()}x{root.winfo_screenheight()}+0+0\")\n",
    "\n",
    "# Styling configuration\n",
    "style = ttk.Style()\n",
    "style.theme_use('clam')\n",
    "style.configure(\"TFrame\", background=BG_COLOR)\n",
    "style.configure(\"TLabel\", background=BG_COLOR, foreground=FG_COLOR)\n",
    "style.configure(\"TButton\", background=BUTTON_COLOR, foreground=FG_COLOR, font=(\"Arial\", 12), relief=\"flat\", padding=8)\n",
    "style.map(\"TButton\", background=[(\"active\", BUTTON_HOVER_COLOR)])\n",
    "\n",
    "# Display for rollout images\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "fig.patch.set_facecolor(BG_COLOR)\n",
    "ax.set_facecolor(BG_COLOR)\n",
    "canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True, pady=(20, 10))\n",
    "\n",
    "def update_observation_image(obs_image):\n",
    "    ax.clear()\n",
    "    ax.imshow(obs_image)\n",
    "    ax.axis('off')\n",
    "    canvas.draw()\n",
    "\n",
    "# Frame for sliders positioned to the right and centered below the image\n",
    "slider_frame = ttk.Frame(root)\n",
    "slider_frame.pack(side=tk.TOP, pady=10)\n",
    "\n",
    "action_labels = [\"Steer\", \"Acceleration\", \"Brake\"]\n",
    "action_ranges = [(-1, 1), (0, 1), (0, 1)]\n",
    "action = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32, device=device)\n",
    "sliders = []\n",
    "\n",
    "# Spacer to push sliders to the right\n",
    "spacer = ttk.Frame(slider_frame, width=200, style=\"TFrame\")\n",
    "spacer.pack(side=tk.LEFT)\n",
    "\n",
    "# Action sliders with custom ranges and names\n",
    "for i in range(3):\n",
    "    label = ttk.Label(slider_frame, text=action_labels[i], font=(\"Arial\", 12, \"bold\"))\n",
    "    label.pack(side=tk.LEFT, padx=(20, 10))\n",
    "\n",
    "    slider = tk.Scale(slider_frame, from_=action_ranges[i][0], to=action_ranges[i][1], resolution=0.01, orient=tk.HORIZONTAL,\n",
    "                      length=300, bg=BG_COLOR, fg=FG_COLOR, troughcolor=SLIDER_COLOR, sliderrelief=\"flat\",\n",
    "                      highlightthickness=0, activebackground=SLIDER_THUMB_COLOR)\n",
    "    slider.set(action[i].item())\n",
    "    slider.pack(side=tk.LEFT, padx=(0, 20))\n",
    "    sliders.append(slider)\n",
    "\n",
    "# Step function\n",
    "def step():\n",
    "    global recurrent_state, latent_state, action\n",
    "    action_values = [slider.get() for slider in sliders]\n",
    "    action = torch.tensor(action_values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Rollout step\n",
    "    next_recurrent_state, next_latent_state, next_observation, next_reward = dreamer.rolloutStep(\n",
    "        recurrent_state, latent_state, action\n",
    "    )\n",
    "    recurrent_state, latent_state = next_recurrent_state, next_latent_state\n",
    "\n",
    "    # Convert observation to image and display\n",
    "    obs_image = next_observation.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    obs_image = np.clip(obs_image * 255, 0, 255).astype(np.uint8)\n",
    "    update_observation_image(obs_image)\n",
    "\n",
    "# Close (X) button in the top right corner\n",
    "close_button = ttk.Button(root, text=\"X\", command=root.destroy, style=\"TButton\")\n",
    "close_button.place(relx=0.98, rely=0.02, anchor=\"ne\")  # Position in top-right corner\n",
    "\n",
    "# Step button below sliders\n",
    "step_button = ttk.Button(root, text=\"Step\", command=step, style=\"TButton\")\n",
    "step_button.pack(side=tk.TOP, pady=20)\n",
    "\n",
    "# Initial display\n",
    "update_observation_image(start_image)\n",
    "\n",
    "# Run GUI\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original = selectedEpisodeObservations[0][1:].cpu()\n",
    "reconstructed = dreamer.reconstructObservations(selectedEpisodeObservations[0], selectedEpisodeActions[0]).cpu()\n",
    "sideBySide = F.interpolate(torch.cat([original, reconstructed], dim=-1), size=(512, 1024), mode='bilinear')\n",
    "saveVideoFrom4DTensor(sideBySide, f\"results/sideBySideRepresentation_{runName}.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedEpisodeObservations[0].shape, selectedEpisodeActions[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
