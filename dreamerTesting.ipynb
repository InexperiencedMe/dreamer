{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from utils import *\n",
    "\n",
    "# Neural Networks\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, representationSize, actionSize, recurrentStateSize):\n",
    "        super().__init__()\n",
    "        self.representationSize = representationSize\n",
    "        self.actionSize = actionSize\n",
    "        self.recurrentStateSize = recurrentStateSize\n",
    "        self.recurrent = nn.GRUCell(representationSize + actionSize, recurrentStateSize)\n",
    "\n",
    "    def forward(self, representation, action, recurrentState):\n",
    "        return self.recurrent(torch.cat((representation, action), -1), recurrentState)\n",
    "    \n",
    "    def initializeRecurrentState(self):\n",
    "        return torch.zeros(self.recurrentStateSize)\n",
    "\n",
    "class PriorNet(nn.Module):\n",
    "    def __init__(self, inputSize, representationClasses=16):\n",
    "        super().__init__()\n",
    "        self.representationSize = representationClasses\n",
    "        self.mlp = sequentialModel1D(inputSize, [256, 256], representationClasses**2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.mlp(x)\n",
    "        stateMatrix = logits.view(-1, self.representationSize, self.representationSize)\n",
    "        _, indices = torch.max(stateMatrix, dim=-1)\n",
    "        representation = F.one_hot(indices, num_classes=self.representationSize)\n",
    "        return representation.view(-1), logits\n",
    "    \n",
    "class PosteriorNet(nn.Module):\n",
    "    def __init__(self, inputSize, representationClasses=16):\n",
    "        super().__init__()\n",
    "        self.representationSize = representationClasses\n",
    "        self.mlp = sequentialModel1D(inputSize, [256, 256], representationClasses**2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.mlp(x)\n",
    "        stateMatrix = logits.view(-1, self.representationSize, self.representationSize)\n",
    "        _, indices = torch.max(stateMatrix, dim=-1)\n",
    "        representation = F.one_hot(indices, num_classes=self.representationSize)\n",
    "        return representation.view(-1), logits\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, inputShape, outputSize):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        c, h, w = inputShape\n",
    "        self.convolutionalNet = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=4, stride=2, padding=1),  # Output: (32, h/2, w/2)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, h/4, w/4)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, h/8, w/8)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Output: (256, h/16, w/16)\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * (h // 16) * (w // 16), outputSize),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.convolutionalNet(obs/255.0)\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, inputSize, outputShape):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.outputShape = outputShape\n",
    "        c, h, w = outputShape\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(inputSize, 256 * (h // 16) * (w // 16)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.deconvolutionalNet = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, h/8, w/8)\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, h/4, w/4)\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Output: (32, h/2, w/2)\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(32, c, kernel_size=4, stride=2, padding=1),  # Output: (c, h, w)\n",
    "            nn.Sigmoid(),  # Output pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        batchSize = x.size(0)\n",
    "        c, h, w = 256, self.outputShape[1] // 16, self.outputShape[2] // 16\n",
    "        x = x.view(batchSize, c, h, w)\n",
    "        return self.deconvolutionalNet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from utils import *\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('episode_0_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "observations = data['observations'].to(device)\n",
    "actions = data['actions'].float().to(device)\n",
    "episodeLength = len(actions)\n",
    "\n",
    "representationClasses = 16\n",
    "representationSize = representationClasses ** 2\n",
    "actionSize = 3\n",
    "recurrentStateSize = 256\n",
    "compressedObservationsSize = 256\n",
    "obsShape = (3, 96, 96)\n",
    "\n",
    "convEncoder     = ConvEncoder(obsShape, compressedObservationsSize).to(device)\n",
    "convDecoder     = ConvDecoder(representationSize + recurrentStateSize, obsShape).to(device)\n",
    "sequenceModel   = SequenceModel(representationSize, actionSize, recurrentStateSize).to(device)\n",
    "priorNet        = PriorNet(recurrentStateSize, representationClasses).to(device)\n",
    "posteriorNet    = PosteriorNet(recurrentStateSize + compressedObservationsSize, representationClasses).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedObservations = convEncoder(observations)\n",
    "recurrentState = sequenceModel.initializeRecurrentState().to(device)\n",
    "\n",
    "posteriorNetOutputs = []\n",
    "recurrentStates = [recurrentState]\n",
    "priorNetLogits = []\n",
    "posteriorNetLogits = []\n",
    "\n",
    "for timestep in range(episodeLength):\n",
    "    posteriorNetOutput, posteriorNetCurrentLogits = posteriorNet(torch.cat((recurrentStates[timestep], encodedObservations[timestep]), -1))\n",
    "    posteriorNetOutputs.append(posteriorNetOutput)\n",
    "    posteriorNetLogits.append(posteriorNetCurrentLogits)\n",
    "\n",
    "    recurrentState = sequenceModel(posteriorNetOutputs[timestep].detach(), actions[timestep], recurrentStates[timestep])\n",
    "    recurrentStates.append(recurrentState)\n",
    "\n",
    "    _, priorNetCurrentLogits = priorNet(recurrentStates[timestep])\n",
    "    priorNetLogits.append(priorNetCurrentLogits)\n",
    "\n",
    "posteriorNetOutputs = torch.stack(posteriorNetOutputs)  # [episodeLength    , representationSize]\n",
    "recurrentStates = torch.stack(recurrentStates)          # [episodeLength + 1, recurrentStateSize]\n",
    "priorNetLogits = torch.stack(priorNetLogits)            # [episodeLength    , representationSize]\n",
    "posteriorNetLogits = torch.stack(posteriorNetLogits)      # [episodeLength    , representationSize]\n",
    "fullStateRepresentations = torch.cat((recurrentStates[1:], posteriorNetOutputs), -1)\n",
    "\n",
    "reconstructedObservations = convDecoder(fullStateRepresentations)\n",
    "\n",
    "reconstructionLoss = F.mse_loss(reconstructedObservations, observations[1:], reduction=\"none\").mean(dim=[-1, -2, -3]).mean() # why is it 255 and 256. In theory we should be able to reconstruct all 256\n",
    "priorNetLoss = F.mse_loss(priorNetLogits, posteriorNetLogits.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructionLoss.mean(dim=[-1, -2, -3]).mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 96, 96])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayImage(np.transpose(observations.cpu().numpy()[50], (1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
