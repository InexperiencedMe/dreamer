{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [1, 0, 0, 1, 0, 2]\n",
      "Values: [0.5, 0.6, 0.4, 0.7, 0.9, 0.8, 0.0]\n",
      "Lambda-returns: tensor([3.4506, 2.5741, 2.7159, 2.8509, 1.9206, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    n = len(rewards)\n",
    "    returns = torch.zeros(n)\n",
    "    bootstrap = values[-1]\n",
    "    for t in reversed(range(n)):\n",
    "        returns[t] = rewards[t] + gamma*((1 - lambda_)*values[t + 1] + lambda_*bootstrap)\n",
    "        bootstrap = returns[t]\n",
    "    return returns\n",
    "\n",
    "rewards = [1, 0, 0, 1, 0, 2]\n",
    "values = [0.5, 0.6, 0.4, 0.7, 0.9, 0.8, 0.0]\n",
    "\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "lmbdaReturns = lambdaReturns(rewards, values, gamma, lambda_)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Values:\", values)\n",
    "print(\"Lambda-returns:\", lmbdaReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 255, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn((4, 255, 3))\n",
    "torch.stack((x, x), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda-returns: tensor([2.5676, 1.6352, 1.7176, 1.7894, 0.7920])\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor([1, 0, 0, 1, 0, 2])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "rewards = rewards[:-1]\n",
    "next_values = values[1:]\n",
    "last = next_values[-1]\n",
    "horizonLength = 6\n",
    "gamma = 0.99\n",
    "inputs = rewards + gamma * next_values * (1 - lambda_)\n",
    "\n",
    "outputs = []\n",
    "# single step\n",
    "for index in reversed(range(horizonLength - 1)):\n",
    "    last = inputs[index] + gamma * lambda_ * last\n",
    "    outputs.append(last)\n",
    "returns = torch.stack(list(reversed(outputs)))\n",
    "print(\"Lambda-returns:\", returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [1, 0, 0, 1, 0, 2]\n",
      "Values: [0.5, 0.6, 0.4, 0.7, 0.9, 0.8]\n",
      "Lambda-returns: tensor([2.5676, 1.6352, 1.7176, 1.7894, 0.7920])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    horizonLength = len(rewards)\n",
    "    bootstrap = values[-1]\n",
    "    returns = torch.zeros(horizonLength - 1)\n",
    "    for t in reversed(range(horizonLength - 1)):\n",
    "        returns[t] = rewards[t] + gamma*((1 - lambda_)*values[t + 1] + lambda_*bootstrap)\n",
    "        bootstrap = returns[t]\n",
    "    return returns\n",
    "\n",
    "rewards = [1, 0, 0, 1, 0, 2]\n",
    "values = [0.5, 0.6, 0.4, 0.7, 0.9, 0.8]\n",
    "\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "lmbdaReturns = lambdaReturns(rewards, values, gamma, lambda_)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Values:\", values)\n",
    "print(\"Lambda-returns:\", lmbdaReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda returns: tensor([1.6676, 1.6884, 1.7741, 0.8394, 0.7920])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, nextValues, lambda_=0.95, gamma=0.99):\n",
    "    td_targets = rewards + gamma * nextValues\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    bootstrap = td_targets[-1]  # Initialize with last TD target\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        returns[t] = (1 - lambda_) * td_targets[t] + lambda_ * bootstrap\n",
    "        bootstrap = rewards[t] + gamma * returns[t]\n",
    "    return returns\n",
    "\n",
    "# Example usage\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0., 2.])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "# Use all but last reward, and all values\n",
    "lambda_returns = lambdaReturns(rewards[:-1], values[1:])\n",
    "print(\"Lambda returns:\", lambda_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returns: tensor([2.5545, 1.6265, 1.6978, 1.7842, 0.7970])\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor([1., 0., 0., 1., 0., 2.])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "horizonLength = len(rewards)\n",
    "returns = torch.zeros_like(rewards[:-1])\n",
    "\n",
    "bootstrap = values[-1]\n",
    "for i in reversed(range(len(returns))):\n",
    "    returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "    bootstrap = returns[i]\n",
    "\n",
    "print(f\"returns: {returns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5964, 1.6591, 1.7201, 1.7951, 0.8026, 0.8000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    # 1 less reward than values, last value is the bootstrap\n",
    "    # I GET IT NOW, USUALLY THEY HAVE 1 FEWER RETURN THAN VALUES BECAUSE THE BOOTSTRAP IS USELESS FOR LOSS CALC AS IT WOULD BE DELTA BETWEEN V_T and V_T\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    returns[-1] = bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "        bootstrap = returns[i]\n",
    "    return returns\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "lambdaReturns = lambdaValues(rewards, values)\n",
    "lambdaReturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial bootstrap (V_T): 0.800000011920929\n",
      "Initial returns[-1] set to bootstrap: 0.800000011920929\n",
      "\n",
      "Step i=4:\n",
      "rewards[i]: 0.0, values[i]: 0.8999999761581421, bootstrap: 0.800000011920929\n",
      "  Update: 0.8025850057601929\n",
      "  Returns[i]: 0.8025850057601929\n",
      "  Updated bootstrap: 0.8025850057601929\n",
      "\n",
      "Step i=3:\n",
      "rewards[i]: 1.0, values[i]: 0.699999988079071, bootstrap: 0.8025850057601929\n",
      "  Update: 1.7950633764266968\n",
      "  Returns[i]: 1.7950633764266968\n",
      "  Updated bootstrap: 1.7950633764266968\n",
      "\n",
      "Step i=2:\n",
      "rewards[i]: 0.0, values[i]: 0.4000000059604645, bootstrap: 1.7950633764266968\n",
      "  Update: 1.7201342582702637\n",
      "  Returns[i]: 1.7201342582702637\n",
      "  Updated bootstrap: 1.7201342582702637\n",
      "\n",
      "Step i=1:\n",
      "rewards[i]: 0.0, values[i]: 0.6000000238418579, bootstrap: 1.7201342582702637\n",
      "  Update: 1.659135103225708\n",
      "  Returns[i]: 1.659135103225708\n",
      "  Updated bootstrap: 1.659135103225708\n",
      "\n",
      "Step i=0:\n",
      "rewards[i]: 1.0, values[i]: 0.5, bootstrap: 1.659135103225708\n",
      "  Update: 2.596374750137329\n",
      "  Returns[i]: 2.596374750137329\n",
      "  Updated bootstrap: 2.596374750137329\n"
     ]
    }
   ],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    # 1 less reward than values, last value is the bootstrap\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    print(f\"Initial bootstrap (V_T): {bootstrap.item()}\")\n",
    "    returns[-1] = bootstrap\n",
    "    print(f\"Initial returns[-1] set to bootstrap: {returns[-1].item()}\")\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        print(f\"\\nStep i={i}:\")\n",
    "        print(f\"rewards[i]: {rewards[i]}, values[i]: {values[i]}, bootstrap: {bootstrap}\")\n",
    "        update = rewards[i] + gamma * ((1 - lambda_) * values[i] + lambda_ * bootstrap)\n",
    "        print(f\"  Update: {update.item()}\")\n",
    "        returns[i] = update\n",
    "        bootstrap = returns[i]\n",
    "        print(f\"  Returns[i]: {returns[i].item()}\")\n",
    "        print(f\"  Updated bootstrap: {bootstrap.item()}\")\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "lambdaReturns = lambdaValues(rewards, values)\n",
    "lambdaReturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6098, 1.6680, 1.7401, 1.8003, 0.7976])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_lambda_values(\n",
    "    rewards,\n",
    "    values,\n",
    "    gamma = 0.997,\n",
    "    lmbda = 0.95,\n",
    "):\n",
    "    vals = [values[-1:]]\n",
    "    interm = rewards + gamma * values * (1 - lmbda)\n",
    "    for t in reversed(range(len(values))):\n",
    "        vals.append(interm[t] + gamma * lmbda * vals[-1])\n",
    "    ret = torch.cat(list(reversed(vals))[:-1])\n",
    "    return ret\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "compute_lambda_values(rewards, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5964, 1.6591, 1.7201, 1.7951, 0.8026, 0.8000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    returns[-1] = bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "        bootstrap = returns[i]\n",
    "    return returns\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "lambdaValues(rewards, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "  Rewards: tensor([5.6989, 3.9511, 4.8963, 7.0852, 3.7286, 8.6404, 4.8623, 8.1160, 5.1036,\n",
      "        0.9738], device='cuda:0')\n",
      "  Updated EMA Values: tensor([0.2213, 1.7404], device='cuda:0')\n",
      "  Offset: 0.22134535014629364, Scale: 1.5190964937210083\n",
      "\n",
      "Batch 2:\n",
      "  Rewards: tensor([5.6851, 5.4815, 6.5169, 5.9742, 5.8991, 6.4466, 5.5185, 5.9447, 8.4563,\n",
      "        7.2663], device='cuda:0')\n",
      "  Updated EMA Values: tensor([0.7490, 2.3585], device='cuda:0')\n",
      "  Offset: 0.7490271925926208, Scale: 1.6094470024108887\n",
      "\n",
      "Batch 3:\n",
      "  Rewards: tensor([4.8100, 1.1124, 8.8829, 4.9382, 0.5479, 1.6851, 7.1290, 1.7753, 5.9428,\n",
      "        6.0926], device='cuda:0')\n",
      "  Updated EMA Values: tensor([0.7543, 2.9320], device='cuda:0')\n",
      "  Offset: 0.7543126344680786, Scale: 2.1776814460754395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class RewardEMA:\n",
    "    \"\"\"running mean and std\"\"\"\n",
    "\n",
    "    def __init__(self, device, alpha=1e-2):\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.range = torch.tensor([0.05, 0.95], device=device)\n",
    "\n",
    "    def __call__(self, x, ema_vals):\n",
    "        flat_x = torch.flatten(x.detach())\n",
    "        x_quantile = torch.quantile(input=flat_x, q=self.range)\n",
    "        # this should be in-place operation\n",
    "        ema_vals[:] = self.alpha * x_quantile + (1 - self.alpha) * ema_vals\n",
    "        scale = torch.clip(ema_vals[1] - ema_vals[0], min=1.0)\n",
    "        offset = ema_vals[0]\n",
    "        return offset.detach(), scale.detach()\n",
    "\n",
    "# Test the RewardEMA class\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize RewardEMA instance\n",
    "reward_ema = RewardEMA(device=device, alpha=0.1)\n",
    "\n",
    "# Initialize EMA values (e.g., starting with some range)\n",
    "ema_vals = torch.tensor([0.0, 1.0], device=device)\n",
    "\n",
    "# Simulate some reward data\n",
    "rewards = [\n",
    "    torch.randn(10, device=device) * 2 + 5,  # Batch 1\n",
    "    torch.randn(10, device=device) * 1 + 7,  # Batch 2\n",
    "    torch.randn(10, device=device) * 3 + 4,  # Batch 3\n",
    "]\n",
    "\n",
    "# Test RewardEMA on each batch\n",
    "for i, batch in enumerate(rewards, 1):\n",
    "    offset, scale = reward_ema(batch, ema_vals)\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Rewards: {batch}\")\n",
    "    print(f\"  Updated EMA Values: {ema_vals}\")\n",
    "    print(f\"  Offset: {offset}, Scale: {scale}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28322812601091227648., 231429933891584., 43300628275493076992., 69045759010565783552., 29312823230201856., 2139703778599763968.,\n",
      "        21612387583658033152., 168346156609306624., 9697492445868064768., 320402916114432., 114128362070343680., 7349037863519911936.,\n",
      "        12908250017415823360., 165277196317884416., 13387510772137984.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# Your log-probabilities tensor\n",
    "logprobs = torch.tensor([44.7902, 33.0753, 45.2147, 45.6813, 37.9168, 42.2072, \n",
    "                         44.5198, 39.6648, 43.7184, 33.4006, 39.2761, 43.4411, \n",
    "                         44.0044, 39.6464, 37.1331])\n",
    "\n",
    "# Convert log-probabilities to probabilities using softmax\n",
    "probabilities = torch.exp(logprobs)\n",
    "\n",
    "# Print probabilities\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
