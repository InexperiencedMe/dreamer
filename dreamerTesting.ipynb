{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [1, 0, 0, 1, 0, 2]\n",
      "Values: [0.5, 0.6, 0.4, 0.7, 0.9, 0.8, 0.0]\n",
      "Lambda-returns: tensor([3.4506, 2.5741, 2.7159, 2.8509, 1.9206, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    n = len(rewards)\n",
    "    returns = torch.zeros(n)\n",
    "    bootstrap = values[-1]\n",
    "    for t in reversed(range(n)):\n",
    "        returns[t] = rewards[t] + gamma*((1 - lambda_)*values[t + 1] + lambda_*bootstrap)\n",
    "        bootstrap = returns[t]\n",
    "    return returns\n",
    "\n",
    "rewards = [1, 0, 0, 1, 0, 2]\n",
    "values = [0.5, 0.6, 0.4, 0.7, 0.9, 0.8, 0.0]\n",
    "\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "lmbdaReturns = lambdaReturns(rewards, values, gamma, lambda_)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Values:\", values)\n",
    "print(\"Lambda-returns:\", lmbdaReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda-returns: tensor([2.5676, 1.6352, 1.7176, 1.7894, 0.7920])\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor([1, 0, 0, 1, 0, 2])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "rewards = rewards[:-1]\n",
    "next_values = values[1:]\n",
    "last = next_values[-1]\n",
    "horizonLength = 6\n",
    "gamma = 0.99\n",
    "inputs = rewards + gamma * next_values * (1 - lambda_)\n",
    "\n",
    "outputs = []\n",
    "# single step\n",
    "for index in reversed(range(horizonLength - 1)):\n",
    "    last = inputs[index] + gamma * lambda_ * last\n",
    "    outputs.append(last)\n",
    "returns = torch.stack(list(reversed(outputs)))\n",
    "print(\"Lambda-returns:\", returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [1, 0, 0, 1, 0, 2]\n",
      "Values: [0.5, 0.6, 0.4, 0.7, 0.9, 0.8]\n",
      "Lambda-returns: tensor([2.5676, 1.6352, 1.7176, 1.7894, 0.7920])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    horizonLength = len(rewards)\n",
    "    bootstrap = values[-1]\n",
    "    returns = torch.zeros(horizonLength - 1)\n",
    "    for t in reversed(range(horizonLength - 1)):\n",
    "        returns[t] = rewards[t] + gamma*((1 - lambda_)*values[t + 1] + lambda_*bootstrap)\n",
    "        bootstrap = returns[t]\n",
    "    return returns\n",
    "\n",
    "rewards = [1, 0, 0, 1, 0, 2]\n",
    "values = [0.5, 0.6, 0.4, 0.7, 0.9, 0.8]\n",
    "\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "lmbdaReturns = lambdaReturns(rewards, values, gamma, lambda_)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Values:\", values)\n",
    "print(\"Lambda-returns:\", lmbdaReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda returns: tensor([1.6676, 1.6884, 1.7741, 0.8394, 0.7920])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, nextValues, lambda_=0.95, gamma=0.99):\n",
    "    td_targets = rewards + gamma * nextValues\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    bootstrap = td_targets[-1]  # Initialize with last TD target\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        returns[t] = (1 - lambda_) * td_targets[t] + lambda_ * bootstrap\n",
    "        bootstrap = rewards[t] + gamma * returns[t]\n",
    "    return returns\n",
    "\n",
    "# Example usage\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0., 2.])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "# Use all but last reward, and all values\n",
    "lambda_returns = lambdaReturns(rewards[:-1], values[1:])\n",
    "print(\"Lambda returns:\", lambda_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returns: tensor([2.5545, 1.6265, 1.6978, 1.7842, 0.7970])\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor([1., 0., 0., 1., 0., 2.])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "horizonLength = len(rewards)\n",
    "returns = torch.zeros_like(rewards[:-1])\n",
    "\n",
    "bootstrap = values[-1]\n",
    "for i in reversed(range(len(returns))):\n",
    "    returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "    bootstrap = returns[i]\n",
    "\n",
    "print(f\"returns: {returns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5964, 1.6591, 1.7201, 1.7951, 0.8026, 0.8000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    # 1 less reward than values, last value is the bootstrap\n",
    "    # I GET IT NOW, USUALLY THEY HAVE 1 FEWER RETURN THAN VALUES BECAUSE THE BOOTSTRAP IS USELESS FOR LOSS CALC AS IT WOULD BE DELTA BETWEEN V_T and V_T\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    returns[-1] = bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "        bootstrap = returns[i]\n",
    "    return returns\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "lambdaReturns = lambdaValues(rewards, values)\n",
    "lambdaReturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial bootstrap (V_T): 0.800000011920929\n",
      "Initial returns[-1] set to bootstrap: 0.800000011920929\n",
      "\n",
      "Step i=4:\n",
      "rewards[i]: 0.0, values[i]: 0.8999999761581421, bootstrap: 0.800000011920929\n",
      "  Update: 0.8025850057601929\n",
      "  Returns[i]: 0.8025850057601929\n",
      "  Updated bootstrap: 0.8025850057601929\n",
      "\n",
      "Step i=3:\n",
      "rewards[i]: 1.0, values[i]: 0.699999988079071, bootstrap: 0.8025850057601929\n",
      "  Update: 1.7950633764266968\n",
      "  Returns[i]: 1.7950633764266968\n",
      "  Updated bootstrap: 1.7950633764266968\n",
      "\n",
      "Step i=2:\n",
      "rewards[i]: 0.0, values[i]: 0.4000000059604645, bootstrap: 1.7950633764266968\n",
      "  Update: 1.7201342582702637\n",
      "  Returns[i]: 1.7201342582702637\n",
      "  Updated bootstrap: 1.7201342582702637\n",
      "\n",
      "Step i=1:\n",
      "rewards[i]: 0.0, values[i]: 0.6000000238418579, bootstrap: 1.7201342582702637\n",
      "  Update: 1.659135103225708\n",
      "  Returns[i]: 1.659135103225708\n",
      "  Updated bootstrap: 1.659135103225708\n",
      "\n",
      "Step i=0:\n",
      "rewards[i]: 1.0, values[i]: 0.5, bootstrap: 1.659135103225708\n",
      "  Update: 2.596374750137329\n",
      "  Returns[i]: 2.596374750137329\n",
      "  Updated bootstrap: 2.596374750137329\n"
     ]
    }
   ],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    # 1 less reward than values, last value is the bootstrap\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    print(f\"Initial bootstrap (V_T): {bootstrap.item()}\")\n",
    "    returns[-1] = bootstrap\n",
    "    print(f\"Initial returns[-1] set to bootstrap: {returns[-1].item()}\")\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        print(f\"\\nStep i={i}:\")\n",
    "        print(f\"rewards[i]: {rewards[i]}, values[i]: {values[i]}, bootstrap: {bootstrap}\")\n",
    "        update = rewards[i] + gamma * ((1 - lambda_) * values[i] + lambda_ * bootstrap)\n",
    "        print(f\"  Update: {update.item()}\")\n",
    "        returns[i] = update\n",
    "        bootstrap = returns[i]\n",
    "        print(f\"  Returns[i]: {returns[i].item()}\")\n",
    "        print(f\"  Updated bootstrap: {bootstrap.item()}\")\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "lambdaReturns = lambdaValues(rewards, values)\n",
    "lambdaReturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6098, 1.6680, 1.7401, 1.8003, 0.7976])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_lambda_values(\n",
    "    rewards,\n",
    "    values,\n",
    "    gamma = 0.997,\n",
    "    lmbda = 0.95,\n",
    "):\n",
    "    vals = [values[-1:]]\n",
    "    interm = rewards + gamma * values * (1 - lmbda)\n",
    "    for t in reversed(range(len(values))):\n",
    "        vals.append(interm[t] + gamma * lmbda * vals[-1])\n",
    "    ret = torch.cat(list(reversed(vals))[:-1])\n",
    "    return ret\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "compute_lambda_values(rewards, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5964, 1.6591, 1.7201, 1.7951, 0.8026, 0.8000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    returns[-1] = bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "        bootstrap = returns[i]\n",
    "    return returns\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "lambdaValues(rewards, values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
