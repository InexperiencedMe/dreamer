{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    n = len(rewards)\n",
    "    returns = torch.zeros(n)\n",
    "    bootstrap = values[-1]\n",
    "    for t in reversed(range(n)):\n",
    "        returns[t] = rewards[t] + gamma*((1 - lambda_)*values[t + 1] + lambda_*bootstrap)\n",
    "        bootstrap = returns[t]\n",
    "    return returns\n",
    "\n",
    "rewards = [1, 0, 0, 1, 0, 2]\n",
    "values = [0.5, 0.6, 0.4, 0.7, 0.9, 0.8, 0.0]\n",
    "\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "lmbdaReturns = lambdaReturns(rewards, values, gamma, lambda_)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Values:\", values)\n",
    "print(\"Lambda-returns:\", lmbdaReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn((4, 255, 3))\n",
    "torch.stack((x, x), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([1, 0, 0, 1, 0, 2])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "rewards = rewards[:-1]\n",
    "next_values = values[1:]\n",
    "last = next_values[-1]\n",
    "horizonLength = 6\n",
    "gamma = 0.99\n",
    "inputs = rewards + gamma * next_values * (1 - lambda_)\n",
    "\n",
    "outputs = []\n",
    "# single step\n",
    "for index in reversed(range(horizonLength - 1)):\n",
    "    last = inputs[index] + gamma * lambda_ * last\n",
    "    outputs.append(last)\n",
    "returns = torch.stack(list(reversed(outputs)))\n",
    "print(\"Lambda-returns:\", returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, values, gamma=0.99, lambda_=0.95):\n",
    "    horizonLength = len(rewards)\n",
    "    bootstrap = values[-1]\n",
    "    returns = torch.zeros(horizonLength - 1)\n",
    "    for t in reversed(range(horizonLength - 1)):\n",
    "        returns[t] = rewards[t] + gamma*((1 - lambda_)*values[t + 1] + lambda_*bootstrap)\n",
    "        bootstrap = returns[t]\n",
    "    return returns\n",
    "\n",
    "rewards = [1, 0, 0, 1, 0, 2]\n",
    "values = [0.5, 0.6, 0.4, 0.7, 0.9, 0.8]\n",
    "\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "lmbdaReturns = lambdaReturns(rewards, values, gamma, lambda_)\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"Values:\", values)\n",
    "print(\"Lambda-returns:\", lmbdaReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def lambdaReturns(rewards, nextValues, lambda_=0.95, gamma=0.99):\n",
    "    td_targets = rewards + gamma * nextValues\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    bootstrap = td_targets[-1]  # Initialize with last TD target\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        returns[t] = (1 - lambda_) * td_targets[t] + lambda_ * bootstrap\n",
    "        bootstrap = rewards[t] + gamma * returns[t]\n",
    "    return returns\n",
    "\n",
    "# Example usage\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0., 2.])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "# Use all but last reward, and all values\n",
    "lambda_returns = lambdaReturns(rewards[:-1], values[1:])\n",
    "print(\"Lambda returns:\", lambda_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([1., 0., 0., 1., 0., 2.])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "horizonLength = len(rewards)\n",
    "returns = torch.zeros_like(rewards[:-1])\n",
    "\n",
    "bootstrap = values[-1]\n",
    "for i in reversed(range(len(returns))):\n",
    "    returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "    bootstrap = returns[i]\n",
    "\n",
    "print(f\"returns: {returns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    # 1 less reward than values, last value is the bootstrap\n",
    "    # I GET IT NOW, USUALLY THEY HAVE 1 FEWER RETURN THAN VALUES BECAUSE THE BOOTSTRAP IS USELESS FOR LOSS CALC AS IT WOULD BE DELTA BETWEEN V_T and V_T\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    returns[-1] = bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "        bootstrap = returns[i]\n",
    "    return returns\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "lambdaReturns = lambdaValues(rewards, values)\n",
    "lambdaReturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    # 1 less reward than values, last value is the bootstrap\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    print(f\"Initial bootstrap (V_T): {bootstrap.item()}\")\n",
    "    returns[-1] = bootstrap\n",
    "    print(f\"Initial returns[-1] set to bootstrap: {returns[-1].item()}\")\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        print(f\"\\nStep i={i}:\")\n",
    "        print(f\"rewards[i]: {rewards[i]}, values[i]: {values[i]}, bootstrap: {bootstrap}\")\n",
    "        update = rewards[i] + gamma * ((1 - lambda_) * values[i] + lambda_ * bootstrap)\n",
    "        print(f\"  Update: {update.item()}\")\n",
    "        returns[i] = update\n",
    "        bootstrap = returns[i]\n",
    "        print(f\"  Returns[i]: {returns[i].item()}\")\n",
    "        print(f\"  Updated bootstrap: {bootstrap.item()}\")\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "lambdaReturns = lambdaValues(rewards, values)\n",
    "lambdaReturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_lambda_values(\n",
    "    rewards,\n",
    "    values,\n",
    "    gamma = 0.997,\n",
    "    lmbda = 0.95,\n",
    "):\n",
    "    vals = [values[-1:]]\n",
    "    interm = rewards + gamma * values * (1 - lmbda)\n",
    "    for t in reversed(range(len(values))):\n",
    "        vals.append(interm[t] + gamma * lmbda * vals[-1])\n",
    "    ret = torch.cat(list(reversed(vals))[:-1])\n",
    "    return ret\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "compute_lambda_values(rewards, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaValues(rewards, values, gamma=0.997, lambda_=0.95):\n",
    "    returns = torch.zeros_like(values)\n",
    "    bootstrap = values[-1]\n",
    "    returns[-1] = bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        returns[i] = rewards[i] + gamma * ((1 - lambda_)*values[i] + lambda_*bootstrap)\n",
    "        bootstrap = returns[i]\n",
    "    return returns\n",
    "\n",
    "rewards = torch.tensor([1., 0., 0., 1., 0.,])\n",
    "values = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.8])\n",
    "\n",
    "lambdaValues(rewards, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RewardEMA:\n",
    "    \"\"\"running mean and std\"\"\"\n",
    "\n",
    "    def __init__(self, device, alpha=1e-2):\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.range = torch.tensor([0.05, 0.95], device=device)\n",
    "\n",
    "    def __call__(self, x, ema_vals):\n",
    "        flat_x = torch.flatten(x.detach())\n",
    "        x_quantile = torch.quantile(input=flat_x, q=self.range)\n",
    "        # this should be in-place operation\n",
    "        ema_vals[:] = self.alpha * x_quantile + (1 - self.alpha) * ema_vals\n",
    "        scale = torch.clip(ema_vals[1] - ema_vals[0], min=1.0)\n",
    "        offset = ema_vals[0]\n",
    "        return offset.detach(), scale.detach()\n",
    "\n",
    "# Test the RewardEMA class\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize RewardEMA instance\n",
    "reward_ema = RewardEMA(device=device, alpha=0.1)\n",
    "\n",
    "# Initialize EMA values (e.g., starting with some range)\n",
    "ema_vals = torch.tensor([0.0, 1.0], device=device)\n",
    "\n",
    "# Simulate some reward data\n",
    "rewards = [\n",
    "    torch.randn(10, device=device) * 2 + 5,  # Batch 1\n",
    "    torch.randn(10, device=device) * 1 + 7,  # Batch 2\n",
    "    torch.randn(10, device=device) * 3 + 4,  # Batch 3\n",
    "]\n",
    "\n",
    "# Test RewardEMA on each batch\n",
    "for i, batch in enumerate(rewards, 1):\n",
    "    offset, scale = reward_ema(batch, ema_vals)\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Rewards: {batch}\")\n",
    "    print(f\"  Updated EMA Values: {ema_vals}\")\n",
    "    print(f\"  Offset: {offset}, Scale: {scale}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# Your log-probabilities tensor\n",
    "logprobs = torch.tensor([44.7902, 33.0753, 45.2147, 45.6813, 37.9168, 42.2072, \n",
    "                         44.5198, 39.6648, 43.7184, 33.4006, 39.2761, 43.4411, \n",
    "                         44.0044, 39.6464, 37.1331])\n",
    "\n",
    "# Convert log-probabilities to probabilities using softmax\n",
    "probabilities = torch.exp(logprobs)\n",
    "\n",
    "# Print probabilities\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
