{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from utils import *\n",
    "\n",
    "# Neural Networks\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, representationSize, actionSize, recurrentStateSize):\n",
    "        super().__init__()\n",
    "        self.recurrent = nn.GRUCell(representationSize + actionSize, recurrentStateSize)\n",
    "\n",
    "    def forward(self, representation, action, recurrentState):\n",
    "        recurrentState = self.recurrent(torch.cat((representation, action), -1), recurrentState)\n",
    "        return recurrentState\n",
    "\n",
    "class PriorNet(nn.Module):\n",
    "    def __init__(self, inputSize, representationClasses=16):\n",
    "        super().__init__()\n",
    "        self.representationSize = representationClasses\n",
    "        self.mlp = sequentialModel1D(inputSize, [256, 256], representationClasses**2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = x.view(-1, self.representationSize, self.representationSize)\n",
    "        _, indices = torch.max(x, dim=-1)\n",
    "        representation = F.one_hot(indices, num_classes=self.representationSize)\n",
    "        return representation\n",
    "    \n",
    "class PosteriorNet(nn.Module):\n",
    "    def __init__(self, inputSize, representationClasses=16):\n",
    "        super().__init__()\n",
    "        self.representationSize = representationClasses\n",
    "        self.mlp = sequentialModel1D(inputSize, [256, 256], representationClasses**2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = x.view(-1, self.representationSize, self.representationSize)\n",
    "        _, indices = torch.max(x, dim=-1)\n",
    "        representation = F.one_hot(indices, num_classes=self.representationSize)\n",
    "        return representation\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, inputShape, outputSize):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        c, h, w = inputShape\n",
    "        self.convolutionalNet = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=4, stride=2, padding=1),  # Output: (32, h/2, w/2)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, h/4, w/4)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, h/8, w/8)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Output: (256, h/16, w/16)\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * (h // 16) * (w // 16), outputSize),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.convolutionalNet(obs/255.0)\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, inputSize, outputShape):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.outputShape = outputShape\n",
    "        c, h, w = outputShape\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(inputSize, 256 * (h // 16) * (w // 16)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.deconvolutionalNet = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, h/8, w/8)\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, h/4, w/4)\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Output: (32, h/2, w/2)\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(32, c, kernel_size=4, stride=2, padding=1),  # Output: (c, h, w)\n",
    "            nn.Sigmoid(),  # Output pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        batch_size = x.size(0)\n",
    "        c, h, w = 256, self.obs_shape[1] // 16, self.obs_shape[2] // 16\n",
    "        x = x.view(batch_size, c, h, w)\n",
    "        return self.deconvolutionalNet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from utils import *\n",
    "import pickle\n",
    "\n",
    "with open('episode_0_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "observations = data['observations']\n",
    "actions = data['actions']\n",
    "\n",
    "representationClasses = 16\n",
    "representationSize = representationClasses ** 2\n",
    "actionSize = 3\n",
    "recurrentStateSize = 256\n",
    "compressedObservationsSize = 256\n",
    "obsShape = (3, 84, 96)\n",
    "\n",
    "convEncoder     = ConvEncoder(obsShape, compressedObservationsSize)\n",
    "convDecoder     = ConvDecoder(representationSize + recurrentStateSize, obsShape)\n",
    "sequenceModel   = SequenceModel(representationSize, actionSize, recurrentStateSize)\n",
    "priorNet        = PriorNet(recurrentStateSize, representationClasses)\n",
    "posteriorNet    = PosteriorNet(recurrentStateSize + compressedObservationsSize, representationClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedObservations = convEncoder(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayImage(np.transpose(observations[50], (1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
